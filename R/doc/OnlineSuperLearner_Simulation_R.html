<!DOCTYPE html>

<html>
<head>
<meta charset="UTF-8">

<title>OnlineSuperLearner.Simulation.R - RDoc Documentation</title>

<script type="text/javascript">
  var rdoc_rel_prefix = "./";
</script>

<script src="./js/jquery.js"></script>
<script src="./js/darkfish.js"></script>

<link href="./css/fonts.css" rel="stylesheet">
<link href="./css/rdoc.css" rel="stylesheet">



<body id="top" role="document" class="file">
<nav role="navigation">
  <div id="project-navigation">
    <div id="home-section" role="region" title="Quick navigation" class="nav-section">
  <h2>
    <a href="./index.html" rel="home">Home</a>
  </h2>

  <div id="table-of-contents-navigation">
    <a href="./table_of_contents.html#pages">Pages</a>
    <a href="./table_of_contents.html#classes">Classes</a>
    <a href="./table_of_contents.html#methods">Methods</a>
  </div>
</div>

    <div id="search-section" role="search" class="project-section initially-hidden">
  <form action="#" method="get" accept-charset="utf-8">
    <div id="search-field-wrapper">
      <input id="search-field" role="combobox" aria-label="Search"
             aria-autocomplete="list" aria-controls="search-results"
             type="text" name="search" placeholder="Search" spellcheck="false"
             title="Type to search, Up and Down to navigate, Enter to load">
    </div>

    <ul id="search-results" aria-label="Search Results"
        aria-busy="false" aria-expanded="false"
        aria-atomic="false" class="initially-hidden"></ul>
  </form>
</div>

  </div>

  

  <div id="project-metadata">
    <div id="fileindex-section" class="nav-section">
  <h3>Pages</h3>

  <ul class="link-list">
  
    <li><a href="./OnlineSuperLearner_Simulation_R.html">OnlineSuperLearner.Simulation.R</a>
  
  </ul>
</div>

  </div>
</nav>

<main role="main" aria-label="Page OnlineSuperLearner.Simulation.R">

<p>#&#39; OnlineSuperLearner.Simulation runs different simulations on the
superlearner object. #&#39; You can run an example using: #&#39;
suppressWarnings(devtools::load_all()); OnlineSuperLearner.Simulation$new()
#&#39; #&#39; @docType class #&#39; @importFrom R6 R6Class #&#39;
@importFrom doParallel registerDoParallel #&#39; @importFrom foreach
foreach #&#39; @include RandomVariable.R #&#39; @include
OutputPlotGenerator.R #&#39; @include OneStepEstimator.R #&#39; @include
SMGFactory.R #&#39; @include SMG.Latest.Entry.R #&#39; @include SMG.Lag.R
#&#39; @include SMG.Transformation.R #&#39; @include
InterventionEffectCalculator.R #&#39; @export OnlineSuperLearner.Simulation
&lt;- R6Class(“OnlineSuperLearner.Simulation”,</p>

<pre>public =
  list(
      initialize = function(configuration = NULL) {
        tic &lt;- Sys.time()
        cat(&#39;Starting calculation with &#39;, parallel::detectCores(),&#39; cores\n&#39;)
        doParallel::registerDoParallel(cores = parallel::detectCores())

        options(warn=1)
        private$sim  &lt;- Simulator.GAD$new()
        private$training_set_size &lt;- 1e4

        OutputPlotGenerator.export_key_value(&#39;training-set-size&#39;, private$training_set_size)
        private$cv_risk_calculator &lt;- CrossValidationRiskCalculator$new()
        private$test_set_size &lt;- 1000
        private$log &lt;- Arguments$getVerbose(-1, timestamp=TRUE)
        #private$log &lt;- FALSE
        #algos &lt;- list(list(description=&#39;ML.H2O.randomForest-1tree&#39;,
                                #algorithm = &#39;ML.H2O.randomForest&#39;,
                                #params = list(ntrees = 1)))

        #algos &lt;- append(algos, list(list(description=&#39;ML.H2O.randomForest-50trees&#39;,
                                #algorithm = &#39;ML.H2O.randomForest&#39;,
                                #params = list(ntrees = 50))))

        #algos &lt;- append(algos, list(list(description=&#39;ML.H2O.gbm&#39;,
                                #algorithm = &#39;ML.H2O.gbm&#39;)))
        nbins &lt;- c(40, 100, 1000)
        algos &lt;- list()

        alphas &lt;- runif(3,0,1)
        alphas &lt;- c(0, alphas)
        alphas &lt;- c(0, 0.961941410787404, 0.523852014588192, 0.193227538373321)

        #lambdas &lt;- runif(3,0,1)
        algos &lt;- append(algos, list(list(algorithm = &#39;ML.XGBoost&#39;,
                                algorithm_params = list(alpha = alphas), 
                                params = list(nbins = nbins, online = FALSE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;ML.H2O.gbm&#39;,
                                #algorithm_params = list(ntrees=c(10,20), min_rows=1),
                                #params = list(nbins = c(6), online = TRUE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;ML.H2O.randomForest&#39;,
                                #algorithm_params = list(ntrees=c(10,20)),
                                #params = list(nbins = nbins, online = TRUE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;ML.SVM&#39;,
                                ##algorithm_params = list(),
                                #params = list(nbins = nbins, online = FALSE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;ML.NeuralNet&#39;,
                                ###algorithm_params = list(),
                                #params = list(nbins = nbins, online = TRUE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;ML.randomForest&#39;,
                                #algorithm_params = list(ntrees=c(500,1000)),
                                #params = list(nbins = nbins, online = FALSE))))

        algos &lt;- append(algos, list(list(algorithm = &#39;ML.Local.Speedlm&#39;,
                                #algorithm_params = list(),
                                params = list(nbins = nbins, online = FALSE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;ML.GLMnet&#39;,
                                ##algorithm_params = list(alpha = alphas),
                                #params = list(nbins = nbins, online = FALSE))))

        #algos &lt;- append(algos, list(list(algorithm = &#39;condensier::glmR6&#39;,
                                ##algorithm_params = list(),
                                #params = list(nbins = c(39, 40), online = FALSE))))

        private$SL.library.definition &lt;- algos

        # Run the simulations
        if(is.null(configuration)) {
          self$configuration2(2)
        } else {
          id = configuration
          configuration = (as.numeric(configuration) - 1) %% 4 + 1 
          if (configuration == 1) {
            self$configuration1(id)
          } else if (configuration == 2) {
            self$configuration2(id)
          } else if (configuration == 3) {
            self$configuration3(id)
          } else {
            self$configuration4(id)
          }
        }

        toc &lt;- Sys.time()
        time.taken &lt;- toc - tic
        print(time.taken)
      },

      configuration1 = function(id) {
        set.seed(12345)

        # Generate the true data generating distributions
        llW &lt;- list(stochMech=function(numberOfBlocks) {
            rnorm(numberOfBlocks, 0, 1)
          },
          param=c(1),
          rgen=identity
        )

        llA &lt;- list(
          stochMech=function(ww) {
            rbinom(length(ww), 1, expit(ww))
          },
          param=c(0.5),
          rgen=function(xx, delta=0.05){
            probability &lt;- delta+(1-2*delta)*expit(xx)
            rbinom(length(xx), 1, probability)
          }
        )

        llY &lt;- list(rgen={function(AW){
            aa &lt;- AW[, &quot;A&quot;]
            ww &lt;- AW[, grep(&quot;[^A]&quot;, colnames(AW))]
            mu &lt;- aa*(0.9) + (1-aa)*(0.3) + rnorm(length(aa), 0, 1)
            mu &lt;- lapply(mu, function(x) max(min(x, 1),0)) %&gt;% unlist
            rbinom(length(aa), 1, mu)
          }}
        )

        # We&#39;d like to use the following features in our estimation:
        W &lt;- RandomVariable$new(formula = W ~ Y_lag_1 + A_lag_1 +  W_lag_1 + Y_lag_2, family = &#39;gaussian&#39;)
        A &lt;- RandomVariable$new(formula = A ~ W + Y_lag_1 + A_lag_1 + W_lag_1, family = &#39;binomial&#39;)
        Y &lt;- RandomVariable$new(formula = Y ~ A + W, family = &#39;binomial&#39;)
        randomVariables &lt;- c(W, A, Y)

        # Generate a dataset we will use for testing.
        # Create the measures we&#39;d like to include in our model
        # In this simulation we will include 2 lags and the latest data (non lagged)
        # Define the variables in the initial dataset we&#39;d like to use
        #private$train(data.test, data.train, bounds, randomVariables, 2)
        intervention &lt;- list(variable = &#39;A&#39;,
                             when = c(3), 
                             what = c(1))
        control &lt;- list(variable = &#39;A&#39;,
                             when = c(3), 
                             what = c(0))

        private$train(intervention = intervention,
                      control = control,
                      randomVariables, Y,  max_iterations = 100,
                      llW = llW,
                      llA = llA, 
                      llY = llY,
                      configuration = id)
      },

      configuration2 = function(id) {
        set.seed(12345)

        ######################################
        # Generate observations for training #
        #####################################
        llW &lt;- list(
          list(stochMech = function(numberOfBlocks) {
              rnorm(numberOfBlocks, 0, 10)
            },
            param=c(1),
            rgen=identity),
          list(stochMech = function(numberOfBlocks) {
              runif(numberOfBlocks, 0, 1)
            },
            param=c(1),
            rgen = identity),
          list(stochMech = function(numberOfBlocks) {
              runif(numberOfBlocks, 0, 1)
            },
            param=c(1),
            rgen = identity)
        )

        llA &lt;- list(
          stochMech=function(ww) {
              rbinom(length(ww), 1, expit(ww))
          },
          #param=c(-0.1, 0.1, 0.3, 0.7),
          param=c(0.5),
          rgen=function(xx, delta=0.05){
            probability &lt;- delta+(1-2*delta)*expit(xx)
            rbinom(length(xx), 1, probability)
          }
        )

        llY &lt;- list(rgen={function(AW){
            aa &lt;- AW[, &quot;A&quot;]
            ww &lt;- AW[, grep(&quot;[^A]&quot;, colnames(AW))]
            mu &lt;- aa*(0.9) + (1-aa)*(0.3) + rnorm(length(aa), 0, 0.01)
            mu &lt;- lapply(mu, function(x) max(min(x, 1),0)) %&gt;% unlist
          }}
        )

        llY &lt;- list(rgen={function(AW){
          aa &lt;- AW[, &quot;A&quot;]
          ww &lt;- AW[, grep(&quot;[^A]&quot;, colnames(AW))]
          mu &lt;- 0.5 * ww + 0.3 * (1-aa) + 0.9 * aa 
          42 + rnorm(length(mu), mu, sd=1)
          }}
        )

        # We&#39;d like to use the following features in our estimation:
        W  &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = W  ~ Y_lag_1 + W2_lag_1 + A_lag_1 +  W_lag_1 + Y_lag_2)
        W2 &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = W2 ~ W_lag_1)
        W3 &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = W3 ~ Y_lag_1)
        A  &lt;- RandomVariable$new(family = &#39;binomial&#39;, formula = A  ~ W + Y_lag_1 + A_lag_1 + W_lag_1)
        Y  &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = Y  ~ A + W + Y_lag_1 + A_lag_1 + W_lag_1)
        randomVariables &lt;- c(W, W2, W3, A, Y)

        # Create the measures we&#39;d like to include in our model
        # In this simulation we will include 2 lags and the latest data (non lagged)
        # Define the variables in the initial dataset we&#39;d like to use

        # Create the measures we&#39;d like to include in our model
        # In this simulation we will include 2 lags and the latest data (non lagged)
        # Define the variables in the initial dataset we&#39;d like to use
        intervention &lt;- list(variable = &#39;A&#39;,
                             when = c(3), 
                             what = c(1))

        control &lt;- list(variable = &#39;A&#39;,
                             when = c(3), 
                             what = c(0))

        private$train(intervention = intervention,
                      control = control,
                      randomVariables, 
                      Y,
                      max_iterations = 10, llW, llA, llY,
                      configuration = id)
      },

      configuration3 = function(id) {
        set.seed(12345)

        # Generate the true data generating distributions
        llW &lt;- list(stochMech=function(numberOfBlocks) {
            rnorm(numberOfBlocks, 0, 1)
          },
          param=c(0, 0.5, -0.25, 0.1),
          rgen=identity
        )

        llA &lt;- list(
          stochMech=function(ww) {
            rbinom(length(ww), 1, expit(ww))
          },
          param=c(-0.1, 0.1, 0.25),
          rgen=function(xx, delta=0.05){
            probability &lt;- delta+(1-2*delta)*expit(xx)
            rbinom(length(xx), 1, probability)
          }
        )

        llY &lt;- list(rgen={function(AW){
            aa &lt;- AW[, &quot;A&quot;]
            ww &lt;- AW[, grep(&quot;[^A]&quot;, colnames(AW))]
            mu &lt;- aa*(0.9) + (1-aa)*(0.3) + rnorm(length(aa), 0, 1)
            mu &lt;- lapply(mu, function(x) max(min(x, 1),0)) %&gt;% unlist
            rbinom(length(aa), 1, mu)
          }}
        )

        # We&#39;d like to use the following features in our estimation:
        W &lt;- RandomVariable$new(formula = W ~ Y_lag_1 + A_lag_1 +  W_lag_1 + Y_lag_2, family = &#39;gaussian&#39;)
        A &lt;- RandomVariable$new(formula = A ~ W + Y_lag_1 + A_lag_1 + W_lag_1, family = &#39;binomial&#39;)
        Y &lt;- RandomVariable$new(formula = Y ~ A + W, family = &#39;binomial&#39;)
        randomVariables &lt;- c(W, A, Y)

        # Create the measures we&#39;d like to include in our model
        # In this simulation we will include 2 lags and the latest data (non lagged)
        # Define the variables in the initial dataset we&#39;d like to use
        #private$train(data.test, data.train, bounds, randomVariables, 2)
        intervention &lt;- list(variable = &#39;A&#39;,
                             when = c(2), 
                             what = c(1))

        control &lt;- list(variable = &#39;A&#39;,
                             when = c(2), 
                             what = c(0))

        private$train(intervention = intervention,
                      control = control,
                      randomVariables,
                      Y,  max_iterations = 100,
                      llW = llW,
                      llA = llA, 
                      llY = llY,
                      configuration = id)
      },

      configuration4 = function(id) {
        set.seed(12345)

        ######################################
        # Generate observations for training #
        #####################################
        llW &lt;- list(
          list(stochMech = function(numberOfBlocks) {
              rnorm(numberOfBlocks, 0, 10)
            },
            param=c(0, 0.5, -0.25, 0.1),
            rgen=identity),
          list(stochMech = function(numberOfBlocks) {
              runif(numberOfBlocks, 0, 1)
            },
            param=c(0, 0.5, -0.25, 0.1),
            rgen = identity),
          list(stochMech = function(numberOfBlocks) {
              runif(numberOfBlocks, 0, 1)
            },
            param=c(0, 0.5, -0.25, 0.1),
            rgen = identity)
        )

        llA &lt;- list(
          stochMech=function(ww) {
              rbinom(length(ww), 1, expit(ww))
          },
          param=c(-0.1, 0.1, 0.3, 0.7),
          rgen=function(xx, delta=0.05){
            probability &lt;- delta+(1-2*delta)*expit(xx)
            rbinom(length(xx), 1, probability)
          }
        )

        llY &lt;- list(rgen={function(AW){
            aa &lt;- AW[, &quot;A&quot;]
            ww &lt;- AW[, grep(&quot;[^A]&quot;, colnames(AW))]
            mu &lt;- aa*(0.9) + (1-aa)*(0.3) + rnorm(length(aa), 0, 0.01)
            mu &lt;- lapply(mu, function(x) max(min(x, 1),0)) %&gt;% unlist
          }}
        )

        llY &lt;- list(rgen={function(AW){
          aa &lt;- AW[, &quot;A&quot;]
          ww &lt;- AW[, grep(&quot;[^A]&quot;, colnames(AW))]
          mu &lt;- 0.5 * ww + 0.3 * (1-aa) + 0.9 * aa 
          42 + rnorm(length(mu), mu, sd=1)
          }}
        )

        # Create the measures we&#39;d like to include in our model
        # In this simulation we will include 2 lags and the latest data (non lagged)
        # Define the variables in the initial dataset we&#39;d like to use
        # We&#39;d like to use the following features in our estimation:
        W  &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = W  ~ Y_lag_1 + W2_lag_1 + A_lag_1 +  W_lag_1 + Y_lag_2)
        W2 &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = W2 ~ W_lag_1)
        W3 &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = W3 ~ Y_lag_1)
        A  &lt;- RandomVariable$new(family = &#39;binomial&#39;, formula = A  ~ W + Y_lag_1 + A_lag_1 + W_lag_1)
        Y  &lt;- RandomVariable$new(family = &#39;gaussian&#39;, formula = Y  ~ A + W + Y_lag_1 + A_lag_1 + W_lag_1)
        randomVariables &lt;- c(W, W2, W3, A, Y)

        # Create the measures we&#39;d like to include in our model
        # In this simulation we will include 2 lags and the latest data (non lagged)
        # Define the variables in the initial dataset we&#39;d like to use

        intervention &lt;- list(variable = &#39;A&#39;,
                             when = c(2), 
                             what = c(1))

        control &lt;- list(variable = &#39;A&#39;,
                             when = c(2), 
                             what = c(0))

        private$train(intervention = intervention,
                      control = control,
                      randomVariables, 
                      Y,
                      max_iterations = 100, llW, llA, llY,
                      configuration = id)
      },

      configuration5 = function() {
      }
),
private =
  list(
      sim = NULL,
      training_set_size = NULL,
      test_set_size = NULL,
      log = NULL,
      SL.library.definition = NULL,
      cv_risk_calculator = NULL,

      train = function(intervention, control, randomVariables, variable_of_interest, max_iterations, llW, llA, llY, configuration) {

        tic &lt;- Sys.time()

        # Initialize variables
        tau &lt;- 3
        B &lt;- 100
        B_oos &lt;- 50
        N &lt;- 10 
        margin &lt;- 100

        result.dosl.mean                 &lt;- -1
        result.dosl_control.mean         &lt;- -1
        result.dosl.mean.updated         &lt;- -1
        result.dosl_control.mean.updated &lt;- -1
        result.osl.mean                  &lt;- -1
        result.osl_control.mean          &lt;- -1
        result.osl.mean.updated          &lt;- -1
        result.osl_control.mean.updated  &lt;- -1
        result.approx.mean               &lt;- -1
        result.approx_control.mean       &lt;- -1
        result.dosl                      &lt;- rep(0.5, B)
        result.dosl_control              &lt;- rep(0.5, B)
        result.osl                       &lt;- rep(0.5, B)
        result.osl_control               &lt;- rep(0.5, B)
        osl_options &lt;- c(&#39;nothing&#39;)
        oos_options &lt;- c(&#39;nothing&#39;)

        # Generate a dataset we will use for testing.
        # We add a margin so we don&#39;t have to worry about the presence of enough history
        data.test &lt;- private$sim$simulateWAY(private$test_set_size + margin, qw=llW, ga=llA, Qy=llY, verbose=private$log)
        data.train &lt;- private$sim$simulateWAY(private$training_set_size + margin, qw=llW, ga=llA, Qy=llY, verbose=private$log)

        # Create the bounds
        bounds &lt;- PreProcessor.generate_bounds(data.train)

        outcome.variables &lt;- sapply(randomVariables, function(rv) rv$getY)
        smg_factory &lt;- SMGFactory$new()

        pre_processor &lt;- PreProcessor$new(bounds = bounds)
        summaryMeasureGenerator &lt;- smg_factory$fabricate(randomVariables, pre_processor = pre_processor)

        Data.Static$new(dataset = data.test) %&gt;%
          summaryMeasureGenerator$setData(.)
        data.test &lt;- summaryMeasureGenerator$getNext(private$test_set_size)

        data.train &lt;- Data.Static$new(dataset = data.train)

        private$log &amp;&amp; cat(private$log, &#39;Initializing OSL&#39;)
        osl &lt;- OnlineSuperLearner$new(private$SL.library.definition,
                                      summaryMeasureGenerator = summaryMeasureGenerator,
                                      pre_processor = pre_processor,
                                      verbose = private$log)

        private$log &amp;&amp; cat(private$log, &#39;Running OSL&#39;)

        initial_training_set_size &lt;- floor(private$training_set_size / 2)
        mini_batch_size &lt;- max((private$training_set_size / 2) / max_iterations, 1)
        mini_batch_size &lt;- ifelse(is.na(mini_batch_size) || is.infinite(mini_batch_size), 1, floor(mini_batch_size))

        # Store the configuration
        key_output = paste(&#39;variables_&#39;, configuration,&#39;.dat&#39;, sep=&#39;&#39;)
        OutputPlotGenerator.export_key_value(&#39;initial-training-set-size&#39;, initial_training_set_size, output=key_output)
        OutputPlotGenerator.export_key_value(&#39;minibatch-size&#39;, mini_batch_size, output=key_output)
        OutputPlotGenerator.export_key_value(&#39;max-iterations&#39;, max_iterations, output=key_output)
        OutputPlotGenerator.export_key_value(&#39;total-iterations&#39;, max_iterations+1, output=key_output)

        # Divide by two here just so the initial size is a lot larger then each iteration, not really important
        risk &lt;- osl$fit(data.train, randomVariables = randomVariables,
                              initial_data_size = initial_training_set_size,
                              max_iterations = max_iterations,
                              mini_batch_size = mini_batch_size) %T&gt;% print

        private$log &amp;&amp; cat(private$log, &#39;Predicting using all estimators&#39;)

        # Calculate prediction quality
        observed.outcome &lt;- data.test[, outcome.variables, with=FALSE]
        predicted.outcome &lt;- osl$predict(data = copy(data.test), randomVariables, plot= TRUE, sample=FALSE)$normalized
        sampled.outcome &lt;- osl$predict(data = copy(data.test), randomVariables, plot= TRUE, sample=TRUE)$normalized

        Evaluation.log_loss(data.predicted = predicted.outcome$osl.estimator$Y, data.observed = observed.outcome$A)
        Evaluation.log_loss(data.predicted = predicted.outcome$dosl.estimator$Y, data.observed = observed.outcome$A)

        performance &lt;- 
          private$cv_risk_calculator$calculate_evaluation(predicted.outcome = predicted.outcome,
                                                          observed.outcome = observed.outcome,
                                                          randomVariables = randomVariables) 

        key_performance = paste(&#39;performance_cfg_&#39;, configuration, sep=&#39;&#39;)
        OutputPlotGenerator.create_risk_plot(performance=performance, output=key_performance)

        key_performance = paste(&#39;risk_cv_cfg_&#39;, configuration, sep=&#39;&#39;)
        OutputPlotGenerator.create_risk_plot(performance=osl$get_cv_risk(), output=key_performance)

        key_performance = paste(&#39;performance_summary_cfg_&#39;, configuration, sep=&#39;&#39;)
        OutputPlotGenerator.create_risk_plot(performance=performance, output=key_performance, make_summary=TRUE, label=&#39;total.evaluation&#39;)

        key_performance = paste(&#39;risk_cv_summary_cfg_&#39;, configuration, sep=&#39;&#39;)
        OutputPlotGenerator.create_risk_plot(performance=osl$get_cv_risk(), output=key_performance, make_summary=TRUE, label=&#39;total.risk&#39;)

        OutputPlotGenerator.create_training_curve(osl$get_historical_cv_risk, 
                                                  randomVariables = randomVariables,
                                                  output = paste(&#39;curve&#39;,configuration, sep=&#39;_&#39;))

        toc &lt;- Sys.time()
        time.taken &lt;- toc - tic
        print(time.taken)

        key &lt;- paste(&#39;cfg&#39;, configuration, &#39;time-taken-training&#39;, sep=&#39;-&#39;)
        key &lt;- paste(&#39;cfg&#39;, configuration, &#39;time-taken-training&#39;, sep=&#39;-&#39;)
        OutputPlotGenerator.export_key_value(key, output=key_output, time.taken)

        tic &lt;- Sys.time()
        data.train$reset
        summaryMeasureGenerator$setData(data.train)
        data.train.set &lt;- summaryMeasureGenerator$getNext(private$training_set_size)

        OutputPlotGenerator.export_key_value(output=key_output, &#39;iterations&#39;, B)
        OutputPlotGenerator.export_key_value(output=key_output, &#39;simulation-number-of-observations&#39;, N)

        intervention_effect_caluculator = InterventionEffectCalculator$new(bootstrap_iterations = B, 
                                                                           randomVariables = randomVariables, 
                                                                           outcome_variable = variable_of_interest$getY,
                                                                           verbose = private$log,
                                                                           parallel = TRUE)

        pre &lt;- options(&#39;warn&#39;)$warn
        options(warn=-1)

        data.aisset &lt;- copy(data.train.set)

        cat(&#39;Approximating truth...\n&#39;)
        result.approx &lt;- foreach(i=seq(B)) %dopar% {
          cat(&#39;Approximating truth in iteration (under intervention): &#39;, i, &#39;\n&#39;)
          data.int &lt;- private$sim$simulateWAY(tau, qw = llW, ga = llA, Qy = llY,
                                            intervention = intervention, verbose = FALSE)
          data.int$Y[tau]
        } %&gt;% unlist

        result.approx_control &lt;- foreach(i=seq(B)) %dopar% {
          cat(&#39;Approximating truth in iteration (under control): &#39;, i, &#39;\n&#39;)
          data.int &lt;- private$sim$simulateWAY(tau, qw = llW, ga = llA, Qy = llY,
                                            intervention = control, verbose = FALSE)
          data.int$Y[tau]
        } %&gt;% unlist

        interventions &lt;- list(intervention = intervention,
                              control = control)

        #osl_options &lt;- c(&#39;dosl&#39;, &#39;osl&#39;)
        osl_options &lt;- c(&#39;dosl&#39;)
        #osl_options &lt;- c(&#39;osl&#39;)

        if(&#39;dosl&#39; %in% osl_options) {
          result.dosl.full &lt;- intervention_effect_caluculator$calculate_intervention_effect(
            osl = osl,
            interventions = interventions, 
            discrete = TRUE, 
            initial_data = data.train.set[1,],
            tau = tau
          )

          result.dosl &lt;- result.dosl.full$intervention          
          result.dosl_control &lt;- result.dosl.full$control
        }

        if(&#39;osl&#39; %in% osl_options) {
          result.osl.full &lt;- intervention_effect_caluculator$calculate_intervention_effect(
            osl = osl,
            interventions = interventions, 
            discrete = FALSE, 
            initial_data = data.train.set[1,],
            tau = tau
          )

          result.osl &lt;- result.osl.full$intervention          
          result.osl_control &lt;- result.osl.full$control
        } 

        ## Plot the convergence
        data &lt;- list(truth = result.approx, dosl = result.dosl, osl = result.osl)
        OutputPlotGenerator.create_convergence_plot(data = data,
                                                    output = paste(&#39;convergence_configuration&#39;,configuration,sep=&#39;_&#39;))

        data &lt;- list(truth = result.approx_control, dosl = result.dosl_control, osl = result.osl_control)
        OutputPlotGenerator.create_convergence_plot(data = data,
                                                    output = paste(&#39;convergence_configuration_control&#39;,configuration,sep=&#39;_&#39;))

        options(warn=pre)

        result.approx.mean &lt;- result.approx %&gt;% mean
        result.approx_control.mean &lt;- result.approx_control %&gt;% mean

        result.dosl.mean &lt;- result.dosl %&gt;% mean
        result.dosl_control.mean &lt;- result.dosl_control %&gt;% mean

        result.osl.mean &lt;- result.osl %&gt;% mean
        result.osl_control.mean &lt;- result.osl_control %&gt;% mean

        osl$info

        ## Retrieve the minimal number of blocks needed before we have truly updating relevant history
        minimal_measurements_needed &lt;- osl$get_summary_measure_generator$get_minimal_measurements_needed
        print(time.taken)

        private$log &amp;&amp; cat(private$log, &#39;Initial estimates!&#39;)
        print(result.approx.mean)
        print(result.approx_control.mean)
        print(result.dosl.mean)
        print(result.dosl_control.mean)
        print(result.osl.mean)
        print(result.osl_control.mean)

        private$log &amp;&amp; cat(private$log, &#39;Starting OOS!&#39;)
        #oos_options &lt;- c(&#39;dosl&#39;, &#39;dosl_control&#39;, &#39;osl&#39;, &#39;osl_control&#39;)
        #oos_options &lt;- c(&#39;dosl&#39;, &#39;dosl_control&#39;)
        # Now, the final step is to apply the OneStepEstimator

        ## DOSL Intervention
        if(&#39;dosl&#39; %in% (oos_options)) {
          OOS.intervention &lt;- OneStepEstimator$new(
            osl = osl, 
            randomVariables = randomVariables, 
            discrete = TRUE,
            N = N, 
            B = B_oos,
            tau = tau,
            intervention = intervention,
            variable_of_interest = variable_of_interest,
            pre_processor = pre_processor,
            minimal_measurements_needed = minimal_measurements_needed
          )

          result.dosl.mean.updated &lt;- OOS.intervention$perform(
            initial_estimate = result.dosl.mean,
            data = data.train.set,
            truth = result.approx.mean
          )$oos_estimate
        }

        ## DOSL Control
        if(&#39;dosl_control&#39; %in% (oos_options)) {
          OOS.control &lt;- OneStepEstimator$new(
                                                      osl = osl, 
                                                      randomVariables = randomVariables, 
                                                      discrete = TRUE,
                                                      N = N, 
                                                      B = B_oos,
                                                      tau = tau,
                                                      intervention = control,
                                                      variable_of_interest = variable_of_interest,
                                                      pre_processor = pre_processor,
            minimal_measurements_needed = minimal_measurements_needed
                                              )

          result.dosl_control.mean.updated &lt;- OOS.control$perform(
                                                      initial_estimate = result.dosl_control.mean,
                                                      data = data.train.set,
                                                      truth = result.approx_control.mean
          )$oos_estimate
        }

        ## OSL Intervention
        if(&#39;osl&#39; %in% (oos_options)) {
          OOS.control &lt;- OneStepEstimator$new(
                                                      osl = osl, 
                                                      randomVariables = randomVariables, 
                                                      discrete = FALSE,
                                                      N = N, 
                                                      B = B_oos,
                                                      tau = tau,
                                                      intervention = intervention,
                                                      variable_of_interest = variable_of_interest,
                                                      pre_processor = pre_processor,
            minimal_measurements_needed = minimal_measurements_needed
                                              )

          result.osl.mean.updated &lt;- OOS.control$perform(
                                                      initial_estimate = result.osl.mean,
                                                      data = data.train.set,
                                                      truth = result.approx.mean
          )$oos_estimate
        }

        ## OSL Control
        if(&#39;osl_control&#39; %in% (oos_options)) {
          OOS.control &lt;- OneStepEstimator$new(
                                                      osl = osl, 
                                                      randomVariables = randomVariables, 
                                                      discrete = FALSE,
                                                      N = N, 
                                                      B = B_oos,
                                                      tau = tau,
                                                      intervention = control,
                                                      variable_of_interest = variable_of_interest,
                                                      pre_processor = pre_processor,
            minimal_measurements_needed = minimal_measurements_needed
                                              )

          result.osl_control.mean.updated &lt;- OOS.control$perform(
                                                      initial_estimate = result.osl_control.mean,
                                                      data = data.train.set,
                                                      truth = result.approx_control.mean
          )$oos_estimate
        }

        estimates &lt;- list(
          dosl             = result.dosl.mean,
          dosl_control     = result.dosl_control.mean,
          dosl_oos         = result.dosl.mean.updated,
          dosl_oos_control = result.dosl_control.mean.updated,

          osl              = result.osl.mean,
          osl_control      = result.osl_control.mean,
          osl_oos          = result.osl.mean.updated,
          osl_oos_control  = result.osl_control.mean.updated,

          approx           = result.approx.mean,
          approx_control   = result.approx_control.mean
        )

        ## Calculate the time it took
        time.taken &lt;- toc - tic
        key &lt;- paste(&#39;cfg&#39;, configuration, &#39;time-taken-oos&#39;, sep=&#39;-&#39;)
        OutputPlotGenerator.export_key_value(key, output=key_output, time.taken)
        print(estimates)

        ## Export the estimates
        lapply(names(estimates), function(entry) {
          key &lt;- paste(&#39;cfg&#39;, configuration, &#39;entry&#39;, entry, sep=&#39;-&#39;)
          OutputPlotGenerator.export_key_value(key, output=key_output, estimates[[entry]])
        })

        estimates
      }
  )</pre>

<p>)</p>
</main>



<footer id="validator-badges" role="contentinfo">
  <p><a href="http://validator.w3.org/check/referer">Validate</a>
  <p>Generated by <a href="http://docs.seattlerb.org/rdoc/">RDoc</a> 4.2.2.
  <p>Based on <a href="http://deveiate.org/projects/Darkfish-RDoc/">Darkfish</a> by <a href="http://deveiate.org">Michael Granger</a>.
</footer>

